{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDgRrdjhhFUB"
   },
   "source": [
    "## Step 1. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXTFbrVHhFUE"
   },
   "outputs": [],
   "source": [
    "# import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, multilabel_confusion_matrix\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, GroupKFold, train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, OneHotEncoder, RobustScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mPPkojMhFUF"
   },
   "source": [
    "## Step 2a. Declare user variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3-fnNiUhFUG"
   },
   "outputs": [],
   "source": [
    "is_test = False\n",
    "is_dataset_one_file = False\n",
    "\n",
    "user_specified_steps = 1\n",
    "user_specified_layout = 'layout1'\n",
    "user_specified_layout_int = int(user_specified_layout[len(user_specified_layout) - 1])\n",
    "user_specified_nth = 3\n",
    "user_specified_clf = 'all'\n",
    "\n",
    "user_test_size = 0.2\n",
    "user_random_state = 1\n",
    "\n",
    "if user_specified_layout_int == 1:\n",
    "    ith_inp_col = 16\n",
    "else:\n",
    "    ith_inp_col = 20\n",
    "\n",
    "list_filenames = {\n",
    "    'layout1':'dataset1.csv',\n",
    "    'layout2':'dataset2.csv',\n",
    "}\n",
    "list_columns = {\n",
    "    'layout1_all':['timestamp','posture_id','posture_label','s01','s02','s03','s04','s05','s06','s07','s08','s09','s10','s11','s12','s13','s14','s15','s16','birth_year','sex','height','weight','bmi','bmi_label','full_name','nth','round'],\n",
    "    'layout2_all':['timestamp','posture_id','posture_label','s01','s02','s03','s04','s05','s06','s07','s08','s09','s10','s11','s12','s13','s14','s15','s16','s17','s18','s19','s20','birth_year','sex','height','weight','bmi','bmi_label','full_name','nth','round'],\n",
    "    'layout1_cat_inp':[],\n",
    "    'layout1_num_inp':['s01','s02','s03','s04','s05','s06','s07','s08','s09','s10','s11','s12','s13','s14','s15','s16', 'height','weight','bmi'],\n",
    "    'layout2_cat_inp':[],\n",
    "    'layout2_num_inp':['s01','s02','s03','s04','s05','s06','s07','s08','s09','s10','s11','s12','s13','s14','s15','s16','s17','s18','s19','s20', 'height','weight','bmi'],\n",
    "}\n",
    "list_positions = ['Yearner_Right', 'Yearner_Left', 'Fetal_Right', 'Fetal_Left', 'Log_Right', 'Log_Left', 'Supine', 'Prone',\n",
    "                  # 'Empty'\n",
    "]\n",
    "\n",
    "filename = list_filenames[user_specified_layout]\n",
    "cols_all = list_columns[user_specified_layout + '_all']\n",
    "cols_cat_inp = list_columns[user_specified_layout + '_cat_inp']\n",
    "cols_num_inp = list_columns[user_specified_layout + '_num_inp']\n",
    "cols_inp = cols_cat_inp + cols_num_inp\n",
    "cols_drp = list(set(cols_all) - set(cols_inp))\n",
    "\n",
    "cols_num_inp_std = ['height','weight','bmi']\n",
    "cols_num_inp_nrm = list(set(cols_num_inp) - set(cols_num_inp_std))\n",
    "\n",
    "col_grp = 'full_name'\n",
    "col_trg = 'posture_id'\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "sns.reset_orig()\n",
    "print(cols_drp) # this print line to be removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIpQ_xVEhFUH"
   },
   "source": [
    "## Step 2b. Declare and prepare needed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if is_dataset_one_file:\n",
    "    df = pd.read_csv(filename, usecols = cols_all)\n",
    "else:\n",
    "    import os\n",
    "    import glob\n",
    "\n",
    "    # os.chdir(\"C:/Users/Julianne/Documents/Notebooks/thesis/data/set_{}\".format(user_specified_layout_int))\n",
    "    os.chdir(\"C:/Users/Julianne/Downloads/set_{} clean\".format(user_specified_layout_int))\n",
    "\n",
    "    extension = 'csv'\n",
    "    list_raw_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "\n",
    "    df = pd.concat([pd.read_csv(f, usecols = cols_all) for f in list_raw_filenames])\n",
    "\n",
    "    df = df[df.nth <= 5]\n",
    "    if(isinstance(user_specified_nth, int))\n",
    "        print('user_specified_nth is an instance of int')\n",
    "        df = df[df.nth == user_specified_nth]\n",
    "\n",
    "    ## export to csv\n",
    "    # combined_csv.to_csv(\"dataset{}.csv\".format(user_specified_layout_int), index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    os.chdir(\"C:/Users/Julianne/Documents/Notebooks/thesis\".format(user_specified_layout_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_test:\n",
    "    df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if is_test:\n",
    "    df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "n2Tq6jamhFUI",
    "outputId": "03ff5da5-6b56-4553-b782-7b0d8d54a977"
   },
   "outputs": [],
   "source": [
    "df_inp = df.drop(columns=cols_drp)\n",
    "\n",
    "X = df_inp\n",
    "\n",
    "y = df[col_trg].values\n",
    "groups = df[col_grp].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_cat_inp_idx = df_inp.columns.get_indexer(cols_cat_inp)\n",
    "cols_num_inp_idx = df_inp.columns.get_indexer(cols_num_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #apply SelectKBest class to extract top 10 best features\n",
    "# bestfeatures = SelectKBest(score_func=chi2, k=10)\n",
    "# fit = bestfeatures.fit(X,y)\n",
    "# dfscores = pd.DataFrame(fit.scores_)\n",
    "# dfcolumns = pd.DataFrame(X.columns)\n",
    "# #concat two dataframes for better visualization \n",
    "# featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "# featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "# print(featureScores.nlargest(10,'Score'))  #print 10 best features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioH2V3TchFUI"
   },
   "source": [
    "## Step 3. Visualize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eI9WFkYJhFUJ"
   },
   "outputs": [],
   "source": [
    "if is_test:\n",
    "    df_inp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMkVIaQRhFUK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pd.plotting.scatter_matrix(X, figsize=(10, 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LF9YPeeyhFUL"
   },
   "outputs": [],
   "source": [
    "if is_test:\n",
    "    # Create correlation matrix\n",
    "    corr_matrix = df.drop(columns=cols_drp).corr()\n",
    "\n",
    "    # Define mask used to cover squares above diagonal\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "    plt.figure(figsize=(20, 10), facecolor='w', edgecolor='k')\n",
    "    plt.title('Correlation Matrix - ' + user_specified_layout)\n",
    "    sns.set(font_scale=1.2)\n",
    "    sns.heatmap(corr_matrix, cmap='coolwarm', center = 0, annot=True, fmt='.1g', mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_cat_inp = [\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "]\n",
    "pipe_cat_inp = Pipeline(steps_cat_inp)\n",
    "\n",
    "steps_num_inp = [\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "#     ('scaler', MinMaxScaler())\n",
    "]\n",
    "pipe_num_inp = Pipeline(steps_num_inp)\n",
    "\n",
    "ct = ColumnTransformer(transformers=[\n",
    "          ('categorical', pipe_cat_inp, cols_cat_inp_idx),\n",
    "          ('numerical', pipe_num_inp, cols_num_inp_idx)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ii7WrvIMhFUL"
   },
   "source": [
    "## Step 4. Split the data into training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCAHqFUlhFUM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=user_test_size, random_state=user_random_state, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[col_grp].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_inp.to_numpy()\n",
    "\n",
    "gkf = GroupKFold(n_splits=df[col_grp].nunique())\n",
    "gkf_split = gkf.split(X, y, groups=groups)\n",
    "for train_index, test_index in gkf_split:\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "#     print(\"%s %s\" % (train_index, test_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBFxrSSahFUM"
   },
   "source": [
    "## Step 5. Declare needed functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confusion_matrix(cnf_matrix, classes, normalize=False, title='Confusion Matrix'):\n",
    "    if normalize:\n",
    "        cnf_matrix = cnf_matrix.astype('float') / cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Confusion Matrix, With Normalized Counts\")\n",
    "        ConfusionMatrixDisplay.from_predictions(actual_targets, predicted_targets, \n",
    "                                                display_labels = list_positions, \n",
    "                                                xticks_rotation = 45, \n",
    "                                                normalize='true', \n",
    "                                                values_format = '.2f')\n",
    "    else:\n",
    "        print(\"Confusion Matrix, Without Normalized Counts\")\n",
    "        ConfusionMatrixDisplay.from_predictions(actual_targets, predicted_targets, \n",
    "                                                display_labels = list_positions, \n",
    "                                                xticks_rotation = 45)\n",
    "\n",
    "#     plt.imshow(cnf_matrix, interpolation='nearest', cmap=plt.get_cmap('Blues'))\n",
    "    plt.title(title)\n",
    "#     plt.colorbar()\n",
    "\n",
    "#     tick_marks = np.arange(len(classes))\n",
    "#     plt.xticks(tick_marks, classes, rotation=45)\n",
    "#     plt.yticks(tick_marks, classes)\n",
    "\n",
    "#     fmt = '.2f' if normalize else 'd'\n",
    "#     thresh = cnf_matrix.max() / 2.\n",
    "\n",
    "#     for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n",
    "#         plt.text(j, i, format(cnf_matrix[i, j], fmt), horizontalalignment=\"center\",\n",
    "#                  color=\"white\" if cnf_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    return cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(actual_targets, predicted_targets, classifier):\n",
    "    cm = confusion_matrix(actual_targets, predicted_targets)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    print(accuracy_score(actual_targets, predicted_targets))\n",
    "    print(precision_score(actual_targets, predicted_targets, average='micro'))\n",
    "    print(recall_score(actual_targets, predicted_targets, average='micro'))\n",
    "    print(f1_score(actual_targets, predicted_targets, average='micro'))\n",
    "    print(classification_report(actual_targets, predicted_targets, target_names=list_positions))\n",
    "#     print(classification_report(actual_targets, predicted_targets, labels=list_positions))\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    generate_confusion_matrix(cm, classes=list_positions, title='%s Confusion Matrix - Layout %x (Without Normalized Counts)' % (classifier, user_specified_layout_int))\n",
    "    plt.savefig('CM-%xS-L%x-%s-nonnormalized.png' % (user_specified_steps, user_specified_layout_int, classifier), format='png', bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    plt.figure()\n",
    "    generate_confusion_matrix(cm, classes=list_positions, normalize=True, title='%s Confusion Matrix - Layout %x (With Normalized Counts)' % (classifier, user_specified_layout_int))\n",
    "    plt.savefig('CM-%xS-L%x-%s-normalized.png' % (user_specified_steps, user_specified_layout_int, classifier), format='png', bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(data_x, data_y, classifier):\n",
    "    group_k_fold = GroupKFold(n_splits=df[col_grp].nunique())\n",
    "\n",
    "    predicted_targets = np.array([])\n",
    "    actual_targets = np.array([])\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(group_k_fold.split(data_x, data_y, groups=groups)):\n",
    "        train_x, train_y, test_x, test_y = data_x[train_index], data_y[train_index], data_x[test_index], data_y[test_index]\n",
    "#         print(\"%s %s\" % (test_x, test_y))\n",
    "\n",
    "        if classifier == 'MLP':\n",
    "            pipe = Pipeline([\n",
    "                ('pre', ct), \n",
    "#                 ('feature_selection', SelectKBest(score_func=f_regression, k=4)),\n",
    "                ('clf', MLPClassifier(max_iter=5000))\n",
    "            ])\n",
    "        elif classifier == 'SVM':\n",
    "            pipe = Pipeline([\n",
    "                ('pre', ct), \n",
    "#                 ('feature_selection', SelectKBest(score_func=f_regression, k=4)),\n",
    "                ('clf', SVC(kernel='rbf', random_state=user_random_state))\n",
    "            ])\n",
    "\n",
    "        cols_identifying_idx = df_inp.columns.get_indexer(['height','weight','bmi'])\n",
    "\n",
    "        pipe.fit(train_x, train_y)\n",
    "        predicted_labels = pipe.predict(test_x)\n",
    "\n",
    "        predicted_targets = np.append(predicted_targets, predicted_labels)\n",
    "        actual_targets = np.append(actual_targets, test_y)\n",
    "\n",
    "#         print('height: %.2f, weight: %.2f, bmi: %.2f' % (test_x[0, cols_identifying_idx[0]], test_x[0, cols_identifying_idx[1]], test_x[0, cols_identifying_idx[2]]))\n",
    "        cmd = ConfusionMatrixDisplay.from_predictions(test_y, predicted_labels, \n",
    "                                                       display_labels = list_positions, \n",
    "                                                       xticks_rotation = 45, \n",
    "                                                       normalize='true', \n",
    "                                                       values_format = '.2f')\n",
    "#         cmd.ax_.set_title('%s Confusion Matrix - Layout %x (height: %.2f, weight: %.2f, bmi: %.2f)' % (classifier, user_specified_layout_int, test_x[0, cols_identifying_idx[0]], test_x[0, cols_identifying_idx[1]], test_x[0, cols_identifying_idx[2]]))\n",
    "        plt.title('%s Confusion Matrix - Layout %x (height: %.2f, weight: %.2f, bmi: %.2f)' % (classifier, user_specified_layout_int, test_x[0, cols_identifying_idx[0]], test_x[0, cols_identifying_idx[1]], test_x[0, cols_identifying_idx[2]]))\n",
    "        plt.tight_layout()\n",
    "#         plt.savefig('kfold confusion matrices/CM-%xS-L%x-%s-height-%.2f, weight-%.2f, bmi-%.2f.png' % (user_specified_steps, user_specified_layout_int, classifier, test_x[0, cols_identifying_idx[0]], test_x[0, cols_identifying_idx[1]], test_x[0, cols_identifying_idx[2]]), format='png', bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "\n",
    "#         plot_confusion_matrix(test_y, predicted_labels)\n",
    "\n",
    "    return actual_targets, predicted_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = X\n",
    "target = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_targets, predicted_targets = evaluate_model(data, target, 'SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(actual_targets, predicted_targets, 'SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_targets, predicted_targets = evaluate_model(data, target, 'MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(actual_targets, predicted_targets, 'MLP')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
