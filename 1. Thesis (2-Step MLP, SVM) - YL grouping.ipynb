{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDgRrdjhhFUB"
   },
   "source": [
    "## Step 1. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jXTFbrVHhFUE"
   },
   "outputs": [],
   "source": [
    "# import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from joblib import dump, load\n",
    "from sklearn import preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, multilabel_confusion_matrix\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, GroupKFold, train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, OneHotEncoder, RobustScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mPPkojMhFUF"
   },
   "source": [
    "## Step 2a. Declare user variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "T3-fnNiUhFUG"
   },
   "outputs": [],
   "source": [
    "is_test = False\n",
    "is_dataset_one_file = False\n",
    "\n",
    "user_specified_layout = 'layout1'\n",
    "user_specified_layout_int = int(user_specified_layout[len(user_specified_layout) - 1])\n",
    "user_specified_nth = 3\n",
    "\n",
    "user_test_size = 0.2\n",
    "user_random_state = 1\n",
    "\n",
    "if user_specified_layout_int == 1:\n",
    "    ith_inp_col = 16\n",
    "else:\n",
    "    ith_inp_col = 20\n",
    "\n",
    "n_steps = 2\n",
    "list_filenames = {\n",
    "    'layout1':'dataset1.csv',\n",
    "    'layout2':'dataset2.csv',\n",
    "}\n",
    "list_columns = {\n",
    "    'layout1_all':['timestamp','posture_id','posture_label','s01','s02','s03','s04','s05','s06','s07','s08','s09','s10','s11','s12','s13','s14','s15','s16','birth_year','sex','height','weight','bmi','bmi_label','full_name','nth','round'],\n",
    "    'layout2_all':['timestamp','posture_id','posture_label','s01','s02','s03','s04','s05','s06','s07','s08','s09','s10','s11','s12','s13','s14','s15','s16','s17','s18','s19','s20','birth_year','sex','height','weight','bmi','bmi_label','full_name','nth','round'],\n",
    "    'layout1_cat_inp':[],\n",
    "    'layout1_num_inp':['s01','s02','s03','s04','s05','s06','s07','s08','s09','s10','s11','s12','s13','s14','s15','s16', 'height','weight','bmi'],\n",
    "    'layout2_cat_inp':[],\n",
    "    'layout2_num_inp':['s01','s02','s03','s04','s05','s06','s07','s08','s09','s10','s11','s12','s13','s14','s15','s16','s17','s18','s19','s20', 'height','weight','bmi'],\n",
    "}\n",
    "list_positions = ['Yearner_Right', 'Yearner_Left', 'Fetal_Right', 'Fetal_Left', 'Log_Right', 'Log_Left', 'Supine', 'Prone',\n",
    "                  # 'Empty'\n",
    "]\n",
    "list_grouped_positions = [\n",
    "    'YRLR',\n",
    "    'YLLL',\n",
    "    'Fetal_Right',\n",
    "    'Fetal_Left',\n",
    "    'Supine',\n",
    "    'Prone',\n",
    "]\n",
    "\n",
    "# {\n",
    "#     'Yearner_Right' : 1,\n",
    "#     'Yearner_Left' : 2,\n",
    "#     'Fetal_Right' : 3,\n",
    "#     'Fetal_Left' : 4,\n",
    "#     'Log_Right' : 5,\n",
    "#     'Log_Left' : 6,\n",
    "#     'Supine' : 7,\n",
    "#     'Prone' : 8,\n",
    "# }\n",
    "\n",
    "filename = list_filenames[user_specified_layout]\n",
    "cols_all = list_columns[user_specified_layout + '_all']\n",
    "cols_cat_inp = list_columns[user_specified_layout + '_cat_inp']\n",
    "cols_num_inp = list_columns[user_specified_layout + '_num_inp']\n",
    "cols_inp = cols_cat_inp + cols_num_inp\n",
    "cols_drp = list(set(cols_all) - set(cols_inp))\n",
    "\n",
    "cols_num_inp_std = ['height','weight','bmi']\n",
    "cols_num_inp_nrm = list(set(cols_num_inp) - set(cols_num_inp_std))\n",
    "\n",
    "col_grp = 'full_name'\n",
    "col_trg = 'posture_id'\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "sns.reset_orig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIpQ_xVEhFUH"
   },
   "source": [
    "## Step 2b. Declare and prepare needed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_specified_nth is an instance of int\n"
     ]
    }
   ],
   "source": [
    "if is_dataset_one_file:\n",
    "    df = pd.read_csv(filename, usecols = cols_all)\n",
    "else:\n",
    "    import os\n",
    "    import glob\n",
    "\n",
    "    # os.chdir(\"C:/Users/Julianne/Documents/Notebooks/thesis/data/set_{}\".format(user_specified_layout_int))\n",
    "    os.chdir(\"C:/Users/Julianne/Downloads/set_{} clean\".format(user_specified_layout_int))\n",
    "\n",
    "    extension = 'csv'\n",
    "    list_raw_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "    df = pd.concat([pd.read_csv(f, usecols = cols_all) for f in list_raw_filenames])\n",
    "\n",
    "    os.chdir(\"C:/Users/Julianne/Documents/Notebooks/thesis\".format(user_specified_layout_int))\n",
    "\n",
    "df = df[df.nth <= 5]\n",
    "\n",
    "# unseen_df_cols_drp = list(set(cols_all) - set(cols_inp) - set(['posture_id','posture_label']))\n",
    "# unseen_df = df[df.nth == 5]\n",
    "# unseen_df = unseen_df.drop(columns=unseen_df_cols_drp)\n",
    "# unseen_df.to_csv(\"clean dataset{} with label.csv\".format(user_specified_layout_int), index=False, encoding='utf-8-sig')\n",
    "# unseen_df = unseen_df.drop(columns=['posture_id','posture_label'])\n",
    "# unseen_df.to_csv(\"clean dataset{}.csv\".format(user_specified_layout_int), index=False, encoding='utf-8-sig')\n",
    "\n",
    "if(isinstance(user_specified_nth, int)):\n",
    "    print('user_specified_nth is an instance of int')\n",
    "    df = df[df.nth == user_specified_nth]\n",
    "    df.to_csv(\"dataset{}.csv\".format(user_specified_layout_int), index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_test:\n",
    "    df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if is_test:\n",
    "#     for col in cols_num_inp_nrm:\n",
    "#         df[col] = df[col] * df['weight']\n",
    "#     df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if is_test:\n",
    "    df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "n2Tq6jamhFUI",
    "outputId": "03ff5da5-6b56-4553-b782-7b0d8d54a977"
   },
   "outputs": [],
   "source": [
    "def categorize(row):\n",
    "    if row['posture_label'] == 'Yearner_Right' or row['posture_label'] == 'Log_Right':\n",
    "        return 100\n",
    "    elif row['posture_label'] == 'Yearner_Left' or row['posture_label'] == 'Log_Left':\n",
    "        return 200\n",
    "    else:\n",
    "        return row['posture_id']\n",
    "\n",
    "df_inp = df.drop(columns=cols_drp)\n",
    "\n",
    "X = df_inp\n",
    "\n",
    "y = df[col_trg].values\n",
    "y_intermediate = df.apply(lambda row: categorize(row), axis=1).values\n",
    "groups = df[col_grp].values\n",
    "\n",
    "# TODO: delete cols_identifying_idx and other calls to this variable\n",
    "cols_identifying_idx = df_inp.columns.get_indexer(['height','weight','bmi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_cat_inp_idx = df_inp.columns.get_indexer(cols_cat_inp)\n",
    "cols_num_inp_idx = df_inp.columns.get_indexer(cols_num_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #apply SelectKBest class to extract top 10 best features\n",
    "# bestfeatures = SelectKBest(score_func=chi2, k=10)\n",
    "# fit = bestfeatures.fit(X,y)\n",
    "# dfscores = pd.DataFrame(fit.scores_)\n",
    "# dfcolumns = pd.DataFrame(X.columns)\n",
    "# #concat two dataframes for better visualization \n",
    "# featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "# featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "# print(featureScores.nlargest(10,'Score'))  #print 10 best features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioH2V3TchFUI"
   },
   "source": [
    "## Step 3. Visualize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "eI9WFkYJhFUJ"
   },
   "outputs": [],
   "source": [
    "if is_test:\n",
    "    df_inp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GMkVIaQRhFUK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pd.plotting.scatter_matrix(X, figsize=(10, 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "LF9YPeeyhFUL"
   },
   "outputs": [],
   "source": [
    "if is_test:\n",
    "    # Create correlation matrix\n",
    "    corr_matrix = df.drop(columns=cols_drp).corr()\n",
    "\n",
    "    # Define mask used to cover squares above diagonal\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "    plt.figure(figsize=(20, 10), facecolor='w', edgecolor='k')\n",
    "    plt.title('Correlation Matrix - ' + user_specified_layout)\n",
    "    sns.set(font_scale=1.2)\n",
    "    sns.heatmap(corr_matrix, cmap='coolwarm', center = 0, annot=True, fmt='.1g', mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_cat_inp = [\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "]\n",
    "pipe_cat_inp = Pipeline(steps_cat_inp)\n",
    "\n",
    "steps_num_inp = [\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "]\n",
    "pipe_num_inp = Pipeline(steps_num_inp)\n",
    "\n",
    "ct = ColumnTransformer(transformers=[\n",
    "          ('categorical', pipe_cat_inp, cols_cat_inp_idx),\n",
    "          ('numerical', pipe_num_inp, cols_num_inp_idx)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[col_grp].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBFxrSSahFUM"
   },
   "source": [
    "## Step 4. Declare needed functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(actual_targets, predicted_targets, target_classes):\n",
    "    print(accuracy_score(actual_targets, predicted_targets))\n",
    "#     print(precision_score(actual_targets, predicted_targets, average='micro'))\n",
    "#     print(recall_score(actual_targets, predicted_targets, average='micro'))\n",
    "#     print(f1_score(actual_targets, predicted_targets, average='micro'))\n",
    "#     try:\n",
    "#         print(classification_report(actual_targets, predicted_targets, target_names=target_classes))\n",
    "#     except:\n",
    "#         print(classification_report(actual_targets, predicted_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confusion_matrix(cnf_matrix, actual_targets, predicted_targets, target_classes, normalize=False, title='Confusion Matrix'):\n",
    "    if normalize:\n",
    "        cnf_matrix = cnf_matrix.astype('float') / cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Confusion Matrix, With Normalized Counts\")\n",
    "        ConfusionMatrixDisplay.from_predictions(actual_targets, predicted_targets, \n",
    "#                                                 display_labels = target_classes, \n",
    "                                                xticks_rotation = 45, \n",
    "                                                normalize='true', \n",
    "                                                values_format = '.2f')\n",
    "    else:\n",
    "        print(\"Confusion Matrix, Without Normalized Counts\")\n",
    "        ConfusionMatrixDisplay.from_predictions(actual_targets, predicted_targets, \n",
    "#                                                 display_labels = target_classes, \n",
    "                                                xticks_rotation = 45)\n",
    "\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    return cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(actual_targets, predicted_targets, classifier, target_classes):\n",
    "    cm = confusion_matrix(actual_targets, predicted_targets)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    print_metrics(actual_targets, predicted_targets, target_classes)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    generate_confusion_matrix(cm, actual_targets, predicted_targets, target_classes=target_classes, title='%s Confusion Matrix - Layout %x (Without Normalized Counts)' % (classifier, user_specified_layout_int))\n",
    "    plt.savefig('CM-%xS-L%x-%s-nonnormalized.png' % (n_steps, user_specified_layout_int, classifier), format='png', bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    plt.figure()\n",
    "    generate_confusion_matrix(cm, actual_targets, predicted_targets, target_classes=target_classes, normalize=True, title='%s Confusion Matrix - Layout %x (With Normalized Counts)' % (classifier, user_specified_layout_int))\n",
    "    plt.savefig('CM-%xS-L%x-%s-normalized.png' % (n_steps, user_specified_layout_int, classifier), format='png', bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pipeline(classifier):\n",
    "    if classifier == 'MLP':\n",
    "        pipe = Pipeline([\n",
    "            ('pre', ct), \n",
    "    #             ('feature_selection', SelectKBest(score_func=chi2, k=10)), # SelectKBest(score_func=f_regression, k=4)\n",
    "            ('clf', MLPClassifier(max_iter=5000))\n",
    "        ])\n",
    "    elif classifier == 'SVM':\n",
    "        pipe = Pipeline([\n",
    "            ('pre', ct), \n",
    "    #             ('feature_selection', SelectKBest(score_func=chi2, k=10)), # SelectKBest(score_func=f_regression, k=4)\n",
    "            ('clf', SVC(kernel='rbf', random_state=user_random_state))\n",
    "        ])\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_final_models(data_x, data_y, data_y_intermediate, classifier):\n",
    "    pipe_intermediate = generate_pipeline(classifier)\n",
    "    pipe_intermediate.fit(data_x, data_y_intermediate)\n",
    "    dump(pipe_intermediate, 'L{}-YearnerLog-Step1.joblib'.format(user_specified_layout_int))\n",
    "\n",
    "    pipe_rights = generate_pipeline(classifier)\n",
    "    pipe_rights.fit(data_x[np.isin(data_y, [1,5])], data_y[np.isin(data_y, [1,5])])\n",
    "    dump(pipe_rights, 'L{}-YearnerLog-Step2rights.joblib'.format(user_specified_layout_int))\n",
    "\n",
    "    pipe_lefts = generate_pipeline(classifier)\n",
    "    pipe_lefts.fit(data_x[np.isin(data_y, [2,6])], data_y[np.isin(data_y, [2,6])])\n",
    "    dump(pipe_lefts, 'L{}-YearnerLog-Step2lefts.joblib'.format(user_specified_layout_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train_x, train_y, test_x, test_y, classifier):\n",
    "    pipe = generate_pipeline(classifier)\n",
    "\n",
    "    pipe.fit(train_x, train_y)\n",
    "    predicted_labels = pipe.predict(test_x)\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_subgroup(train_x, train_y, test_x, test_y, intermediate_labels, list_ids, subgroup_id, classifier):\n",
    "    train_x_subgroup = train_x[np.isin(train_y, list_ids)]\n",
    "    train_y_subgroup = train_y[np.isin(train_y, list_ids)]\n",
    "    test_x_predicted_subgroup = test_x[np.isin(intermediate_labels, subgroup_id)]\n",
    "    test_y_corresponding_predicted_subgroup = test_y[np.isin(intermediate_labels, subgroup_id)]\n",
    "\n",
    "    predicted_labels_from_subgroup = []\n",
    "    if test_x_predicted_subgroup.size:\n",
    "        predicted_labels_from_subgroup = model(train_x_subgroup, train_y_subgroup, \n",
    "                                               test_x_predicted_subgroup, test_y_corresponding_predicted_subgroup, \n",
    "                                               classifier)\n",
    "    else:\n",
    "        print(\"None of the participant's data was classified as subgroup {}\".format(subgroup_id))\n",
    "\n",
    "    return predicted_labels_from_subgroup, test_y_corresponding_predicted_subgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(data_x, data_y, data_y_intermediate, classifier):\n",
    "    group_k_fold = GroupKFold(n_splits=df[col_grp].nunique())\n",
    "\n",
    "    predicted_targets = np.array([])\n",
    "    intermediate_actual_targets = np.array([])\n",
    "    actual_targets = np.array([])\n",
    "    predicted_targets_rights = np.array([])\n",
    "    actual_targets_rights = np.array([])\n",
    "    predicted_targets_lefts = np.array([])\n",
    "    actual_targets_lefts = np.array([])\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(group_k_fold.split(data_x, data_y, groups=groups)):\n",
    "        train_x, test_x = data_x[train_index], data_x[test_index]\n",
    "        train_y, test_y = data_y[train_index], data_y[test_index]\n",
    "        train_y_intermediate, test_y_intermediate = data_y_intermediate[train_index], data_y_intermediate[test_index]\n",
    "        print('fold %x (height: %.2f, weight: %.2f, bmi: %.2f)' % (i, test_x[0, cols_identifying_idx[0]], test_x[0, cols_identifying_idx[1]], test_x[0, cols_identifying_idx[2]]))\n",
    "\n",
    "        intermediate_output = model(train_x, train_y_intermediate, test_x, test_y_intermediate, classifier)\n",
    "\n",
    "        predicted_targets = np.append(predicted_targets, intermediate_output)\n",
    "        intermediate_actual_targets = np.append(intermediate_actual_targets, test_y_intermediate)\n",
    "        actual_targets = np.append(actual_targets, test_y)\n",
    "\n",
    "#         cmd = ConfusionMatrixDisplay.from_predictions(test_y_intermediate, predicted_labels, \n",
    "#                                                       display_labels = list_grouped_positions, \n",
    "#                                                       xticks_rotation = 45, \n",
    "#                                                       normalize='true', \n",
    "#                                                       values_format = '.2f')\n",
    "#         plt.title('%s Confusion Matrix - Layout %x (height: %.2f, weight: %.2f, bmi: %.2f)' % (classifier, user_specified_layout_int, test_x[0, cols_identifying_idx[0]], test_x[0, cols_identifying_idx[1]], test_x[0, cols_identifying_idx[2]]))\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig('kfold confusion matrices/CM-%xS-L%x-%s-height-%.2f, weight-%.2f, bmi-%.2f.png' % (n_steps, user_specified_layout_int, classifier, test_x[0, cols_identifying_idx[0]], test_x[0, cols_identifying_idx[1]], test_x[0, cols_identifying_idx[2]]), format='png', bbox_inches=\"tight\")\n",
    "#         plt.show()\n",
    "\n",
    "        output, corresponding_true_y = evaluate_subgroup(train_x, train_y, test_x, test_y, intermediate_output, [1,5], 100, classifier)\n",
    "\n",
    "        predicted_targets_rights = np.append(predicted_targets_rights, output)\n",
    "        actual_targets_rights = np.append(actual_targets_rights, corresponding_true_y)\n",
    "\n",
    "        output, corresponding_true_y = evaluate_subgroup(train_x, train_y, test_x, test_y, intermediate_output, [2,6], 200, classifier)\n",
    "\n",
    "        predicted_targets_lefts = np.append(predicted_targets_lefts, output)\n",
    "        actual_targets_lefts = np.append(actual_targets_lefts, corresponding_true_y)\n",
    "\n",
    "#         cmd = ConfusionMatrixDisplay.from_predictions(test_y_corresponding_predicted_rights, predicted_labels_2, \n",
    "#                                                       display_labels = list_positions, \n",
    "#                                                       xticks_rotation = 45, \n",
    "#                                                       normalize='true', \n",
    "#                                                       values_format = '.2f')\n",
    "#         plt.title('%s Confusion Matrix - Layout %x (height: %.2f, weight: %.2f, bmi: %.2f)' % (classifier, user_specified_layout_int, test_x[0, cols_identifying_idx[0]], test_x[0, cols_identifying_idx[1]], test_x[0, cols_identifying_idx[2]]))\n",
    "#         plt.tight_layout()\n",
    "# #         plt.savefig('kfold confusion matrices/CM-%xS-L%x-%s-height-%.2f, weight-%.2f, bmi-%.2f.png' % (n_steps, user_specified_layout_int, classifier, test_x[0, cols_identifying_idx[0]], test_x[0, cols_identifying_idx[1]], test_x[0, cols_identifying_idx[2]]), format='png', bbox_inches=\"tight\")\n",
    "#         plt.show()\n",
    "    #TODO: fix target_classes\n",
    "#     plot_confusion_matrix(intermediate_actual_targets, predicted_targets, classifier, list_grouped_positions)\n",
    "#     plot_confusion_matrix(actual_targets_rights, predicted_targets_rights, classifier, list_positions)\n",
    "#     plot_confusion_matrix(actual_targets_lefts, predicted_targets_lefts, classifier, list_positions)\n",
    "\n",
    "    new_predicted_targets = []\n",
    "    ctr_rights = 0\n",
    "    ctr_lefts = 0\n",
    "    for target in predicted_targets:\n",
    "        if target == 100:\n",
    "            new_predicted_targets.append(predicted_targets_rights[ctr_rights])\n",
    "            ctr_rights += 1\n",
    "        elif target == 200:\n",
    "            new_predicted_targets.append(predicted_targets_lefts[ctr_lefts])\n",
    "            ctr_lefts += 1\n",
    "        else:\n",
    "            new_predicted_targets.append(target)\n",
    "            \n",
    "#     np.putmask(predicted_targets, predicted_targets == 100, predicted_targets_rights)\n",
    "#     np.putmask(predicted_targets, predicted_targets == 200, predicted_targets_lefts)\n",
    "#     np.putmask(actual_targets, actual_targets == 100, actual_targets_rights)\n",
    "#     np.putmask(actual_targets, actual_targets == 200, actual_targets_lefts)\n",
    "\n",
    "    return actual_targets, new_predicted_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_inp.to_numpy()\n",
    "\n",
    "data = X\n",
    "target = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_targets, predicted_targets = evaluate_model(data, target, y_intermediate, 'SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_confusion_matrix(actual_targets, predicted_targets, 'SVM', list_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_targets, predicted_targets = evaluate_model(data, target, y_intermediate, 'MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_confusion_matrix(actual_targets, predicted_targets, 'MLP', list_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_final_models(data, target, y_intermediate, 'SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_final_models(data, target, y_intermediate, 'MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'L1-YearnerLog-Step1.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m x_sample \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[0;32m      2\u001b[0m     [\u001b[38;5;241m769\u001b[39m,\u001b[38;5;241m588\u001b[39m,\u001b[38;5;241m485\u001b[39m,\u001b[38;5;241m987\u001b[39m,\u001b[38;5;241m663\u001b[39m,\u001b[38;5;241m567\u001b[39m,\u001b[38;5;241m399\u001b[39m,\u001b[38;5;241m913\u001b[39m,\u001b[38;5;241m956\u001b[39m,\u001b[38;5;241m686\u001b[39m,\u001b[38;5;241m151\u001b[39m,\u001b[38;5;241m785\u001b[39m,\u001b[38;5;241m991\u001b[39m,\u001b[38;5;241m806\u001b[39m,\u001b[38;5;241m157\u001b[39m,\u001b[38;5;241m633\u001b[39m],\n\u001b[0;32m      3\u001b[0m     [\u001b[38;5;241m781\u001b[39m,\u001b[38;5;241m604\u001b[39m,\u001b[38;5;241m475\u001b[39m,\u001b[38;5;241m985\u001b[39m,\u001b[38;5;241m683\u001b[39m,\u001b[38;5;241m590\u001b[39m,\u001b[38;5;241m379\u001b[39m,\u001b[38;5;241m921\u001b[39m,\u001b[38;5;241m934\u001b[39m,\u001b[38;5;241m716\u001b[39m,\u001b[38;5;241m177\u001b[39m,\u001b[38;5;241m791\u001b[39m,\u001b[38;5;241m895\u001b[39m,\u001b[38;5;241m590\u001b[39m,\u001b[38;5;241m121\u001b[39m,\u001b[38;5;241m715\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     [\u001b[38;5;241m845\u001b[39m,\u001b[38;5;241m656\u001b[39m,\u001b[38;5;241m679\u001b[39m,\u001b[38;5;241m817\u001b[39m,\u001b[38;5;241m657\u001b[39m,\u001b[38;5;241m592\u001b[39m,\u001b[38;5;241m548\u001b[39m,\u001b[38;5;241m646\u001b[39m,\u001b[38;5;241m721\u001b[39m,\u001b[38;5;241m251\u001b[39m,\u001b[38;5;241m648\u001b[39m,\u001b[38;5;241m854\u001b[39m,\u001b[38;5;241m800\u001b[39m,\u001b[38;5;241m529\u001b[39m,\u001b[38;5;241m844\u001b[39m,\u001b[38;5;241m985\u001b[39m],\n\u001b[0;32m     10\u001b[0m ])\n\u001b[1;32m---> 12\u001b[0m saved_model \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m-YearnerLog-Step1.joblib\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_specified_layout_int\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m predicted_targets \u001b[38;5;241m=\u001b[39m saved_model\u001b[38;5;241m.\u001b[39mpredict(x_sample)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_sample[np\u001b[38;5;241m.\u001b[39misin(predicted_targets, \u001b[38;5;241m100\u001b[39m)]\u001b[38;5;241m.\u001b[39msize:\n",
      "File \u001b[1;32m~\\Documents\\Notebooks\\thesis\\.venv\\Lib\\site-packages\\joblib\\numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'L1-YearnerLog-Step1.joblib'"
     ]
    }
   ],
   "source": [
    "# x_sample = np.array([\n",
    "#     [769,588,485,987,663,567,399,913,956,686,151,785,991,806,157,633],\n",
    "#     [781,604,475,985,683,590,379,921,934,716,177,791,895,590,121,715],\n",
    "#     [863,588,567,986,877,575,408,944,986,595,139,839,985,808,141,657],\n",
    "#     [942,716,561,864,803,662,464,759,860,376,478,819,714,441,196,860],\n",
    "#     [984,724,308,551,960,617,326,676,933,178,881,912,609,140,982,986],\n",
    "#     [986,712,313,593,924,598,318,699,826,149,857,894,615,128,808,921],\n",
    "#     [985,752,364,656,979,694,259,837,923,162,677,985,573,114,971,985],\n",
    "#     [845,656,679,817,657,592,548,646,721,251,648,854,800,529,844,985],\n",
    "# ])\n",
    "\n",
    "# saved_model = load('L{}-YearnerLog-Step1.joblib'.format(user_specified_layout_int))\n",
    "# predicted_targets = saved_model.predict(x_sample)\n",
    "\n",
    "# if x_sample[np.isin(predicted_targets, 100)].size:\n",
    "#     saved_model_rights = load('L{}-YearnerLog-Step2rights.joblib'.format(user_specified_layout_int))\n",
    "#     predicted_targets_rights = saved_model_rights.predict(x_sample[np.isin(predicted_targets, 100)])\n",
    "\n",
    "# if x_sample[np.isin(predicted_targets, 200)].size:\n",
    "#     saved_model_lefts = load('L{}-YearnerLog-Step2lefts.joblib'.format(user_specified_layout_int))\n",
    "#     predicted_targets_lefts = saved_model_lefts.predict(x_sample[np.isin(predicted_targets, 200)])\n",
    "\n",
    "# new_predicted_targets = []\n",
    "# ctr_rights = 0\n",
    "# ctr_lefts = 0\n",
    "# for target in predicted_targets:\n",
    "#     if target == 100:\n",
    "#         new_predicted_targets.append(predicted_targets_rights[ctr_rights])\n",
    "#         ctr_rights += 1\n",
    "#     elif target == 200:\n",
    "#         new_predicted_targets.append(predicted_targets_lefts[ctr_lefts])\n",
    "#         ctr_lefts += 1\n",
    "#     else:\n",
    "#         new_predicted_targets.append(target)\n",
    "\n",
    "# for i, pred in enumerate(new_predicted_targets):\n",
    "#     print('line #{:''>3}: {:''>3}, {}'.format(i + 2, predicted_targets[i], new_predicted_targets[i]))\n",
    "# # print(predicted_targets)\n",
    "# # print(new_predicted_targets)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
